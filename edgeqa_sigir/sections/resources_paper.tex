\section{Overview}
\label{sec:res_overview}

This paper releases a model-aware QA resource family designed for \emph{coverage diagnostics} and \emph{API-only feasibility}:
\begin{itemize}[leftmargin=*]
  \item \textbf{EdgeQA builder:} an end-to-end pipeline to synthesize evidence-grounded QA pairs from an unlabeled corpus, targeting a specified black-box model's \emph{unknown} and \emph{brittle} knowledge.
  \item \textbf{Coverage evaluation toolkit:} document / knowledge-unit / reasoning-type coverage curves under QA and token budgets.
  \item \textbf{EdgeCoverBench:} a robustness benchmark derived from corpus units, with canonical, paraphrase, near-miss, and unanswerable items, supporting abstention and risk--coverage analysis.
  \item \textbf{IR exports:} BEIR-style test collections (queries/corpus/qrels) created from evidence mappings.
\end{itemize}

\section{EdgeQA in one page}
\label{sec:edgeqa_short}

\paragraph{Goal.}
Given an unlabeled corpus $\mathcal{C}$ and a target black-box model $M$, EdgeQA constructs a grounded QA set $\mathcal{Q}$ that (i) is answerable from corpus evidence, (ii) is \emph{unknown or brittle} for $M$ in closed-book mode, and (iii) maximizes coverage of $\mathcal{C}$ under a budget.

\paragraph{Pipeline.}
EdgeQA (1) segments documents into passages, (2) mines knowledge-rich candidates, (3) generates evidence-grounded QA (single-hop and multi-hop), (4) scores candidates with behavioral signals (closed-book failure, sampling/paraphrase inconsistency), (5) filters for grounding and ambiguity with a fast verifier, and (6) selects $N$ examples by a greedy objective that trades off document/unit/reason coverage and redundancy.

\paragraph{Coverage metrics.}
\label{sec:unitcov}
We report DocCov (fraction of passages exercised by evidence), UnitCov (fraction of extracted knowledge units covered), and ReasonCov (distribution over reasoning/operator tags), each as a function of QA budget $N$ and token budget.

\section{EdgeCoverBench}
\label{sec:ecb_short}

EdgeCoverBench is a unit-anchored stress-test benchmark constructed from the same corpora.
For each canonical item, we generate meaning-preserving paraphrases and ``near-miss'' counterfactuals (minimal constraint edits) that become contradicted or unanswerable from the same evidence.
This enables robustness measurement and abstention evaluation (risk--coverage curves) on a benchmark whose \emph{coverage} can be computed over corpus units.

\section{Released instantiations and validation}
\label{sec:res_results}

\paragraph{Corpora.}
We instantiate the release on two redistributable CC BY 4.0 corpora: Open Logic Project (OLP; LaTeX textbook) and OpenStax University Physics (OSP; HTML textbook).
Evidence text is redistributed with attribution.

\paragraph{Coverage and composition (DeepSeek-V3.2 chat).}
We release EdgeQA at budgets $N\in\{1k,2k,5k,10k\}$ per corpus and report coverage curves in Figure~\ref{fig:coverage_curves}.
Table~\ref{tab:coverage_main} summarizes DocCov/UnitCov and the multi-hop fraction.
\input{tables/tab_coverage_main}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/coverage_curves.pdf}
\caption{Coverage curves for the released instantiations as a function of QA budget $N$.}
\label{fig:coverage_curves}
\end{figure}

\paragraph{EdgeCoverBench size.}
The released EdgeCoverBench contains 10,052 items for OLP (3,187 canonical; 6,370 paraphrase; 495 near-miss; 444 unanswerable) and 6,575 items for OSP (2,089 canonical; 4,176 paraphrase; 310 near-miss; 234 unanswerable).

\paragraph{Human audit (C2).}
To mitigate LLM-as-a-judge bias, we include a targeted manual audit for each corpus (100 EdgeQA items; 100 EdgeCoverBench items).
For EdgeQA, answer correctness is 1.00 on both corpora with evidence support 1.00 (OSP) and 0.98 (OLP) and $\le$1\% ambiguity (Table~\ref{tab:human_audit_edgeqa}).
EdgeCoverBench also achieves 1.00 meaning preservation on paraphrases and 1.00 validity for near-miss labels on both corpora (Table~\ref{tab:human_audit_ecb}); full audit sheets and scripts are released with the resource.
\input{tables/tab_human_audit_edgeqa}
\input{tables/tab_human_audit_edgecoverbench}

\section{Availability (reviewer access)}
\label{sec:res_avail}

\textbf{Resource link:} \texttt{https://huggingface.co/datasets/cactusYuri/edgeqa-resource-release}.
The resource is intended to be publicly accessible without login.
We release: (i) corpus passages and structured units (\texttt{passages.jsonl}, \texttt{units.jsonl}), (ii) EdgeQA JSONL exports (\texttt{edgeqa\_N*.jsonl}), (iii) EdgeCoverBench (\texttt{edgecoverbench.jsonl}), (iv) BEIR exports (\texttt{queries.jsonl}, \texttt{corpus.jsonl}, \texttt{qrels.tsv}), (v) human audit sheets, and (vi) manifests with SHA-256 checksums.

\textbf{Reproducibility:} we include prompt templates, configuration YAMLs, and cached intermediate artifacts and token logs to reproduce all reported figures/tables without re-running API calls.

\textbf{Code link:} \texttt{https://github.com/cactusYuri/edgeqa} (Apache-2.0).
Upstream corpora and evidence text are CC BY 4.0 with attribution preserved.

\section{Conclusion}
EdgeQA and EdgeCoverBench provide a reusable, coverage-oriented framework for constructing and evaluating model-specific unknown and brittle knowledge questions from unlabeled text, with redistributable instantiations and IR exports.

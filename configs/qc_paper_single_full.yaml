run_name: qc_paper_single_full

artifacts_dir: ../artifacts
cache_dir: ../cache

llm:
  # App-level async concurrency (LLM calls). Actual HTTP in-flight is additionally capped
  # in `LLMapi_service/gptservice.py` via `EDGEQA_GATEWAY_MAX_INFLIGHT`.
  concurrency: 2000
  force_refresh: false
  close_session: false
  model_chat: deepseek-chat
  model_reasoner: deepseek-chat

edgeqa:
  # Paper default is 20k per corpus; corpora here have fewer passages, so this effectively means "use all".
  candidate_passages: 20000
  qa_per_passage: 2
  multi_hop_rate: 0.25
  multi_hop_questions: 1
  edge_tau: 0.60
  weights: {w1: 1.0, w2: 1.0, w3: 1.0}
  # We run the max budget once, and derive smaller budgets from its prefix for curves.
  final_N: 10000
  passage_workers: 2000
  pool_write_batch: 64
  done_write_batch: 64

edgecoverbench:
  unit_workers: 2000

decoding:
  gen:           {temperature: 0.7, max_tokens: 256}
  closed_book:   {temperature: 0.0, max_tokens: 64}
  sample:        {temperature: 0.8, max_tokens: 64, m: 4}
  paraphrase:    {temperature: 0.8, max_tokens: 192, k: 2}
  verify_fast:   {temperature: 0.0, max_tokens: 64}
  verify_strict: {temperature: 0.0, max_tokens: 128}

filtering:
  max_question_evidence_jaccard: 0.65
  strict_verify: false
  strict_verify_rate: 0.0

equivalence:
  use_llm: true

selection:
  lambdas: {doc: 1.0, unit: 1.0, reason: 0.5, redundancy: 0.2}
  unknownness_min_frac: 0.5

corpora:
  olp:
    source:
      type: git
      url: https://github.com/OpenLogicProject/OpenLogic.git
      revision: master
  osp:
    source:
      type: git
      url: https://github.com/openstax/university-physics.git
      revision: main
